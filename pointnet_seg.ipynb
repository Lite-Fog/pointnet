{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import h5py\n",
    "import argparse,socket\n",
    "import datetime\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "#!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.6.0\n",
      "keras version: 2.6.0\n",
      "python version: 3.8.0 (default, Jul 23 2021, 11:57:41) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow version:\",tf.__version__)\n",
    "print(\"keras version:\",keras.__version__)\n",
    "print(\"python version:\",sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(False)\n",
    "#disable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPU, 1 Logical GPUs\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 13:46:55.208993: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-26 13:46:55.611180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20000 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices(device_type=None))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Creates virtual GPU with 1GB memory limit\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=20000)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "print(logical_gpus) #AVX stands for Advanced Vector Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /home/liteandfog/pointnet/sem_seg\n"
     ]
    }
   ],
   "source": [
    "# working directories and paths\n",
    "BASE_DIR = os.getcwd()\n",
    "sys.path.append(BASE_DIR)\n",
    "print(\"BASE_DIR:\",BASE_DIR)\n",
    "\n",
    "#global parameters\n",
    "MAX_NUM_POINT = 4096\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCH = 100\n",
    "BASE_LEARNING_RATE =0.001\n",
    "OPTIMIZER = 'adam'\n",
    "MOMENTUM = 0.9\n",
    "DECAY_STEP = 300000\n",
    "DECAY_RATE = 0.5\n",
    "NUM_CLASSES = 13\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "writer = tf.summary.create_file_writer(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    return (data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ALL data\n",
    "ALL_FILES = ['data/'+line.rstrip() for line in open(os.path.join(BASE_DIR, 'data/indoor3d_sem_seg_hdf5_data/all_files.txt'))]\n",
    "room_filelist = ['data/'+line.rstrip() for line in open('data/indoor3d_sem_seg_hdf5_data/room_filelist.txt')]\n",
    "data_batch_list = []\n",
    "label_batch_list = []\n",
    "for h5_filename in ALL_FILES:\n",
    "    data_batch, label_batch = load_h5(h5_filename)\n",
    "    data_batch_list.append(data_batch)\n",
    "    label_batch_list.append(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23585, 4096, 9)\n",
      "(23585, 4096)\n"
     ]
    }
   ],
   "source": [
    "data_batches = np.concatenate(data_batch_list, 0)\n",
    "label_batches = np.concatenate(label_batch_list, 0)\n",
    "print(data_batches.shape)\n",
    "print(label_batches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_area = 'Area_'+str(6)\n",
    "train_idxs = []\n",
    "test_idxs = []\n",
    "for i,room_name in enumerate(room_filelist):\n",
    "    if test_area in room_name:\n",
    "        test_idxs.append(i)\n",
    "    else:\n",
    "        train_idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: (20291, 4096, 9) (20291, 4096)\n",
      "test data: (3294, 4096, 9) (3294, 4096)\n",
      "test/train ratio:0.16233798235670988\n"
     ]
    }
   ],
   "source": [
    "train_data = data_batches[train_idxs,...]\n",
    "train_label = label_batches[train_idxs]\n",
    "test_data = data_batches[test_idxs,...]\n",
    "test_label = label_batches[test_idxs]\n",
    "print(\"training data:\",train_data.shape, train_label.shape)\n",
    "print(\"test data:\",test_data.shape, test_label.shape)\n",
    "print(f\"test/train ratio:{test_data.shape[0]/train_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights_shapes=[[1,9,1,64], [1,1,64,64], [1,1,64,64],[1,1,64,128],[1,1,128,1024],[1024,256],[256,128]]\n",
    "\n",
    "Weights_shapes_1=[[1,1,1152,512],[1,1,512,256],[1,1,256,13]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _kernel(shape):\n",
    "    initializer = keras.initializers.GlorotNormal() #Xavier\n",
    "    with tf.device('/cpu:0'): #save on cpu memory\n",
    "        kernel=tf.Variable(name=\"weights\",initial_value=initializer(shape=shape))\n",
    "    return kernel\n",
    "\n",
    "def _biases(output_channels):\n",
    "    intializer = keras.initializers.Constant(0.0)\n",
    "    with tf.device('/cpu:0'): #save on cpu memory\n",
    "        biases=tf.Variable(name=\"biases\",shape=output_channels,initial_value=intializer(shape=[output_channels]))\n",
    "    return biases\n",
    "\n",
    "def _bn(momentum):\n",
    "    with tf.device('/cpu:0'): #save on cpu memory\n",
    "\n",
    "        bn_init=keras.layers.BatchNormalization(momentum=momentum,\n",
    "           beta_initializer='zeros', gamma_initializer='ones',\n",
    "           moving_mean_initializer='ones', moving_variance_initializer='ones')\n",
    "    return bn_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(keras.layers.Layer):\n",
    "    def __init__(self, shape, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.shape=shape\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.w = _kernel(self.shape)\n",
    "        self.b = _biases(self.shape[-1])\n",
    "        self.bn = _bn(BN_INIT_DECAY)\n",
    "        \n",
    "    def call(self, x, bn=True, activation=True, is_training=True):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        \n",
    "        if bn:\n",
    "            y = self.bn(y,training = is_training)\n",
    "        \n",
    "        if activation:\n",
    "            y = tf.nn.relu(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "class Conv2d(keras.layers.Layer):\n",
    "    def __init__(self, shape, strides=[1,1,1,1], padding='VALID', data_format='NHWC', name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.shape=shape\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.data_format = data_format\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.filters = _kernel(self.shape)\n",
    "        self.bias = _biases(self.shape[-1])\n",
    "        self.bn = tf.keras.layers.BatchNormalization(momentum=0.5,\n",
    "        beta_initializer='zeros', gamma_initializer='ones',\n",
    "        moving_mean_initializer='ones', moving_variance_initializer='ones')\n",
    "        \n",
    "    def call(self, inputs, bn=True, activation=True, is_training=True):\n",
    "        y = tf.nn.conv2d(inputs, filters=self.filters, strides=self.strides, padding=self.padding , data_format=self.data_format)\n",
    "        y = tf.nn.bias_add(y, self.bias)\n",
    "        \n",
    "        if bn:\n",
    "            y = self.bn(y,training = is_training)\n",
    "        \n",
    "        if activation:\n",
    "            y = tf.nn.relu(y)\n",
    "        return y\n",
    "    \n",
    "class MaxPool(keras.layers.Layer):\n",
    "    def __init__(self, batch_size=32 ,name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    def call(self, pcd):\n",
    "        y = tf.nn.max_pool(pcd, ksize=[1, MAX_NUM_POINT, 1, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "        y = tf.reshape(y, [self.batch_size, -1])\n",
    "        return y\n",
    "    \n",
    "class TransformMatrix(keras.layers.Layer):\n",
    "    def __init__(self, shape, K, batch_size=32, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.shape=shape\n",
    "        self.batch_size=batch_size\n",
    "        self.K=K\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = _kernel(self.shape)\n",
    "        bias = _biases(self.shape[-1]).assign_add(tf.constant(np.eye(self.K).flatten(), dtype=tf.float32))\n",
    "        self.b = bias\n",
    "        \n",
    "    def call(self, pcd):\n",
    "        \n",
    "        y = tf.matmul(pcd, self.w)\n",
    "        y = tf.nn.bias_add(y, self.b)\n",
    "        y = tf.reshape(y, [self.batch_size, self.K, self.K])\n",
    "        return y #BxKxK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(PointNet, self).__init__()\n",
    "        self.conv1_1 = Conv2d(Weights_shapes[0])\n",
    "        self.conv1_2 = Conv2d(Weights_shapes[1])  \n",
    "        self.conv1_3 = Conv2d(Weights_shapes[2])\n",
    "        self.conv1_4 = Conv2d(Weights_shapes[3])\n",
    "        self.conv1_5 = Conv2d(Weights_shapes[4])\n",
    "        \n",
    "        self.max_pool = MaxPool()\n",
    "        self.fc_1 = Dense(Weights_shapes[5])\n",
    "        self.fc_2 = Dense(Weights_shapes[6])\n",
    "        \n",
    "        \n",
    "        self.conv2_1 = Conv2d(Weights_shapes_1[0])\n",
    "        self.conv2_2 = Conv2d(Weights_shapes_1[1])  \n",
    "        self.conv2_3 = Conv2d(Weights_shapes_1[2])\n",
    "        \n",
    "        self.dropout=tf.keras.layers.Dropout(rate=0.7)\n",
    "        \n",
    "      \n",
    "    def call(self, inputs, is_training=True):\n",
    "        \n",
    "        batch_size = inputs.shape[0]\n",
    "        num_point = inputs.shape[1]\n",
    "        \n",
    "        net = tf.expand_dims(inputs, -1)\n",
    "        net = self.conv1_1(net, is_training=is_training)\n",
    "        net = self.conv1_2(net, is_training=is_training)\n",
    "        net = self.conv1_3(net, is_training=is_training)\n",
    "        net = self.conv1_4(net, is_training=is_training)\n",
    "        \n",
    "        points_feat1 = self.conv1_5(net, is_training=is_training) \n",
    "        \n",
    "        # MAX\n",
    "        pc_feat1 = self.max_pool(points_feat1)\n",
    "        \n",
    "        # FC\n",
    "        pc_feat1 = tf.reshape(pc_feat1, [batch_size, -1])\n",
    "        pc_feat1 = self.fc_1(pc_feat1,is_training=is_training)\n",
    "        pc_feat1 = self.fc_2(pc_feat1,is_training=is_training)\n",
    "        #print(pc_feat1)\n",
    "           \n",
    "        # CONCAT \n",
    "        pc_feat1_expand = tf.tile(tf.reshape(pc_feat1, [batch_size, 1, 1, -1]), [1, num_point, 1, 1])\n",
    "        points_feat1_concat = tf.concat(axis=3, values=[points_feat1, pc_feat1_expand])    \n",
    "        \n",
    "        #print(points_feat1_concat.shape)\n",
    "        # CONV \n",
    "        net = self.conv2_1(points_feat1_concat,is_training=is_training)\n",
    "        net = self.conv2_2(net,is_training=is_training)\n",
    "        net = self.dropout(net,training=is_training)\n",
    "        net = self.conv2_3(net,is_training=is_training)\n",
    "        net = tf.squeeze(net, [2])\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 13:47:16.895935: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "2021-10-26 13:47:18.438853: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 4096, 13), dtype=float32, numpy=\n",
       "array([[[0.40291032, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.43335667],\n",
       "        [1.2509133 , 0.19500989, 1.6917155 , ..., 0.        ,\n",
       "         1.48139   , 0.        ],\n",
       "        [0.        , 0.3572611 , 0.        , ..., 0.7992417 ,\n",
       "         1.7681752 , 0.8204681 ],\n",
       "        ...,\n",
       "        [0.        , 0.2614817 , 1.6911188 , ..., 0.        ,\n",
       "         0.        , 0.5358056 ],\n",
       "        [0.22568426, 0.        , 1.484625  , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.8759454 , 0.6394088 , 1.0660714 , ..., 0.        ,\n",
       "         0.38041803, 0.        ]],\n",
       "\n",
       "       [[0.        , 0.0492514 , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 1.8799969 , 0.        , ..., 0.88067824,\n",
       "         0.4265388 , 1.222658  ],\n",
       "        [0.24277559, 1.2993528 , 0.        , ..., 0.        ,\n",
       "         0.04485658, 1.7135996 ],\n",
       "        ...,\n",
       "        [0.2189036 , 0.        , 0.30088982, ..., 0.        ,\n",
       "         0.        , 0.7682026 ],\n",
       "        [0.        , 1.0610539 , 0.        , ..., 0.        ,\n",
       "         1.1672611 , 0.4612653 ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.02375591, 0.        ]],\n",
       "\n",
       "       [[0.6528667 , 0.        , 0.        , ..., 0.26833317,\n",
       "         0.        , 1.1764582 ],\n",
       "        [0.25215757, 0.        , 2.0792518 , ..., 1.0265355 ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 1.6615199 , ..., 0.05289452,\n",
       "         0.4667637 , 0.        ],\n",
       "        ...,\n",
       "        [0.5320015 , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.05889542],\n",
       "        [0.        , 1.3176273 , 0.6634579 , ..., 1.8450177 ,\n",
       "         1.0132715 , 0.16347517],\n",
       "        [0.21963967, 1.0745646 , 0.        , ..., 0.1941654 ,\n",
       "         0.        , 1.0313255 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.58935964, 1.0124305 , 0.71385217, ..., 0.56094104,\n",
       "         0.        , 0.2168787 ],\n",
       "        [0.0386971 , 0.        , 0.        , ..., 1.7427706 ,\n",
       "         0.22322406, 0.        ],\n",
       "        [0.        , 0.        , 0.06174561, ..., 0.        ,\n",
       "         0.        , 0.02897025],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.7988668 , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.6779027 , 1.260717  , ..., 0.8923569 ,\n",
       "         1.5142249 , 0.04994661],\n",
       "        [0.        , 2.085395  , 0.        , ..., 0.        ,\n",
       "         1.003197  , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.07994077, 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [1.5195085 , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 1.110729  ],\n",
       "        [0.        , 0.61598897, 0.336382  , ..., 0.16410291,\n",
       "         1.5619724 , 0.20615728],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.4206934 , ..., 0.        ,\n",
       "         1.4063097 , 1.1397246 ],\n",
       "        [0.        , 0.9220025 , 0.        , ..., 0.        ,\n",
       "         0.13977534, 0.        ],\n",
       "        [1.7795839 , 0.36717576, 0.13424245, ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.34671026, 0.        , 0.19421798, ..., 0.        ,\n",
       "         2.3317442 , 1.101484  ],\n",
       "        [1.3318968 , 0.        , 0.        , ..., 0.        ,\n",
       "         1.1934947 , 0.42269167],\n",
       "        [0.231552  , 0.        , 0.        , ..., 1.6505244 ,\n",
       "         0.        , 0.38150772],\n",
       "        ...,\n",
       "        [0.60273385, 0.        , 0.        , ..., 0.48477155,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 2.6547267 , 0.        , ..., 0.17728442,\n",
       "         1.2326727 , 2.0252366 ],\n",
       "        [0.        , 0.        , 1.2968107 , ..., 0.        ,\n",
       "         1.341179  , 0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = train_data[:BATCH_SIZE]\n",
    "label = tf.cast(train_label[:BATCH_SIZE],'int64')\n",
    "model = PointNet()\n",
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"point_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2d)              multiple                  896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2d)            multiple                  4416      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2d)            multiple                  4416      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2d)            multiple                  8832      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2d)            multiple                  136192    \n",
      "_________________________________________________________________\n",
      "max_pool (MaxPool)           multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  263424    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  33408     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2d)            multiple                  592384    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2d)            multiple                  132352    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2d)            multiple                  3393      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "=================================================================\n",
      "Total params: 1,179,713\n",
      "Trainable params: 1,174,695\n",
      "Non-trainable params: 5,018\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(pred, label):\n",
    "    \"\"\" pred: B,N,13\n",
    "        label: B,N \"\"\"\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.7196634>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss(model(batch), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"A LearningRateSchedule that uses an exponential decay schedule.\n",
    "    Returns:\n",
    "      A 1-arg callable learning rate schedule that takes the current optimizer\n",
    "      step and outputs the decayed learning rate, a scalar `Tensor` of the same\n",
    "      type as `initial_learning_rate`.\n",
    "      modification: clips the learning rate !\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,initial_learning_rate,decay_steps,decay_rate,staircase=False,name=None):\n",
    "        \"\"\"Applies exponential decay to the learning rate.\n",
    "        Args:\n",
    "          initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n",
    "            Python number.  The initial learning rate.\n",
    "          decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.\n",
    "            Must be positive.  See the decay computation above.\n",
    "          decay_rate: A scalar `float32` or `float64` `Tensor` or a\n",
    "            Python number.  The decay rate.\n",
    "          staircase: Boolean.  If `True` decay the learning rate at discrete\n",
    "            intervals\n",
    "          name: String.  Optional name of the operation.  Defaults to\n",
    "            'ExponentialDecay'.\n",
    "        \"\"\"\n",
    "        super(ExponentialDecay, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate\n",
    "        self.staircase = staircase\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step=step*BATCH_SIZE\n",
    "        with tf.name_scope(self.name or \"ExponentialDecay\") as name:\n",
    "            initial_learning_rate = tf.convert_to_tensor(\n",
    "                                 self.initial_learning_rate, name=\"initial_learning_rate\")\n",
    "            dtype = initial_learning_rate.dtype\n",
    "            decay_steps = tf.cast(self.decay_steps, dtype)\n",
    "            decay_rate = tf.cast(self.decay_rate, dtype)\n",
    "            global_step_recomp = tf.cast(step, dtype)\n",
    "            p = global_step_recomp / decay_steps\n",
    "            \n",
    "            if self.staircase:\n",
    "                p = tf.floor(p)\n",
    "            \n",
    "            return tf.maximum(tf.multiply(initial_learning_rate, tf.pow(decay_rate, p), name=name), 0.00001)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data, labels):\n",
    "    \"\"\" Shuffle data and labels.\n",
    "        Input:\n",
    "          data: B,N,... numpy array\n",
    "          label: B,... numpy array\n",
    "        Return:\n",
    "          shuffled data, label and shuffle indices\n",
    "    \"\"\"\n",
    "    idx = np.arange(len(labels))\n",
    "    np.random.shuffle(idx)\n",
    "    return data[idx, ...], labels[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch_num=MAX_EPOCH):\n",
    "    \n",
    "    is_training = True\n",
    "    \n",
    "    # learning rate\n",
    "    lr = ExponentialDecay(BASE_LEARNING_RATE, DECAY_STEP, DECAY_RATE, staircase=True)\n",
    "    optimizer = tf.optimizers.Adam(lr)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        print(f\"LOG: EPOCH NUMBER: {epoch}\")\n",
    "        train_one_epoch(epoch, optimizer, is_training)\n",
    "        eval_mean_loss, eval_acc = eval_one_epoch()\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('eval_accuracy',eval_acc , step=epoch)\n",
    "        \n",
    "        \n",
    "    # Save the model\n",
    "    model.save_weights('./model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, optimizer, is_training=True):\n",
    "    \n",
    "    \n",
    "    current_data, current_label, _ = shuffle_data(train_data[:,0:MAX_NUM_POINT,:], train_label)\n",
    "    file_size = current_data.shape[0]\n",
    "    num_batches = file_size // BATCH_SIZE\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Current batch/total batch num: {batch_idx}/{num_batches}')\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "        \n",
    "        \n",
    "        batch = current_data[start_idx:end_idx, :, :]\n",
    "        label = tf.cast(current_label[start_idx:end_idx],'int64')\n",
    "        \n",
    "        pred,loss = train_step(model, batch, label  , optimizer, is_training=is_training)\n",
    "        pred = np.argmax(pred, 2)\n",
    "        correct = np.sum(pred == label)\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE*MAX_NUM_POINT)\n",
    "        loss_sum += loss\n",
    "    \n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('loss', loss_sum/num_batches, step=epoch)\n",
    "        tf.summary.scalar('accuracy',total_correct/total_seen , step=epoch)\n",
    "    \n",
    "    print(f'mean loss: {loss_sum}/{float(num_batches)}')\n",
    "    print(f'accuracy: {total_correct/total_seen}')\n",
    "\n",
    "              \n",
    "def train_step(model, inputs , labels, optimizer, is_training=True):\n",
    "    \n",
    "    #single model iteration for tensorBoard graph's plot.\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(inputs, is_training=is_training)\n",
    "        loss = get_loss(pred, labels)\n",
    "        \n",
    "    grads = tape.gradient(loss , model.trainable_weights)\n",
    "    \n",
    "    optimizer.apply_gradients(\n",
    "    (grad, var) \n",
    "    for (grad, var) in zip(grads, model.trainable_variables) \n",
    "    if grad is not None)\n",
    "    \n",
    "    \n",
    "    return pred,loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch():\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    print('----')\n",
    "    current_data = test_data[:,0:MAX_NUM_POINT,:]\n",
    "    current_label = tf.cast(np.squeeze(test_label),'int64')\n",
    "    \n",
    "    num_batches = current_data.shape[0] // BATCH_SIZE\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        print(batch_idx,num_batches)\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "        \n",
    "        batch = current_data[start_idx:end_idx, :, :]\n",
    "        label = tf.cast(current_label[start_idx:end_idx],'int64')\n",
    "        \n",
    "        pred = model(batch,is_training)\n",
    "        loss = get_loss(pred, label)\n",
    "        \n",
    "        pred = np.argmax(pred, 2)\n",
    "        \n",
    "\n",
    "        correct = np.sum(pred == current_label[start_idx:end_idx])\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE*MAX_NUM_POINT)\n",
    "        loss_sum += (loss*BATCH_SIZE)\n",
    "        \n",
    "        \n",
    "        eval_mean_loss = loss_sum / float(total_seen/MAX_NUM_POINT)\n",
    "        eval_accuracy = total_correct / float(total_seen)\n",
    "        \n",
    "        print(f'eval mean loss: {eval_mean_loss}')\n",
    "        print(f'eval accuracy: {eval_accuracy}')\n",
    "        \n",
    "        return eval_mean_loss, eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8ef4027470c29dfc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8ef4027470c29dfc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir $logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: EPOCH NUMBER: 0\n",
      "Current batch/total batch num: 0/634\n",
      "Current batch/total batch num: 100/634\n",
      "Current batch/total batch num: 200/634\n",
      "Current batch/total batch num: 300/634\n",
      "Current batch/total batch num: 400/634\n",
      "Current batch/total batch num: 500/634\n",
      "Current batch/total batch num: 600/634\n",
      "mean loss: 757.0279541015625/634.0\n",
      "accuracy: 0.7645443456030042\n",
      "----\n",
      "0 102\n",
      "eval mean loss: 0.6077287197113037\n",
      "eval accuracy: 0.91339111328125\n",
      "LOG: EPOCH NUMBER: 1\n",
      "Current batch/total batch num: 0/634\n",
      "Current batch/total batch num: 100/634\n",
      "Current batch/total batch num: 200/634\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
